{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_classes import PYKE\n",
    "from helper_classes import Parser\n",
    "from helper_classes import DataAnalyser\n",
    "from helper_classes import PPMI\n",
    "\n",
    "import util as ut\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning DBpedia Embedings with PYKE\n",
    "\n",
    "1. Download drugbank.nq.gz from http://download.bio2rdf.org/#/release/4/drugbank/\n",
    "2. Extract drugbank.nq and locate the file under KGs/Drugbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODEL PARAMS\n",
    "K = 45\n",
    "num_of_dims = 50\n",
    "bound_on_iter = 30\n",
    "omega = 0.45557\n",
    "e_release = 0.0414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_root = 'KGs/Drugbank'\n",
    "kg_path = kg_root + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Drugbank is serializedin N-Quads format\n",
    "ut.triple=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Preprocessing  starts ######\n",
      "\n",
      "\n",
      "###### Constructing Inverted Index  starts ######\n",
      "Number of RDF triples: 3146309\n",
      "Number of vocabulary terms:  521363\n",
      "Number of subjects:  421121\n",
      "Constructing Inverted Index  took  42.67512631416321  seconds\n",
      "\n",
      "\n",
      "\n",
      "###### Calculation of PPMIs  starts ######\n",
      "Calculation of PPMIs  took  113.38293743133545  seconds\n",
      "\n",
      "Preprocessing  took  156.2215337753296  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "storage_path, experiment_folder = ut.create_experiment_folder()\n",
    "\n",
    "parser = Parser(p_folder=storage_path, k=K)\n",
    "\n",
    "parser.set_similarity_measure(PPMI)\n",
    "\n",
    "model = PYKE()\n",
    "\n",
    "analyser = DataAnalyser(p_folder=storage_path)\n",
    "\n",
    "\n",
    "# For the illustration purpusoes lets consider only the first 1000 ntriples\n",
    "#holder = parser.pipeline_of_preprocessing(kg_path,bound=1000)\n",
    "\n",
    "holder = parser.pipeline_of_preprocessing(kg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the original data contains many literals. \n",
    "# Hence 7 seconds of overhead stems from reading literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Generating Embeddings:  starts ######\n",
      "EPOCH:  0\n",
      "EPOCH:  1\n",
      "EPOCH:  2\n",
      "EPOCH:  3\n",
      "EPOCH:  4\n",
      "EPOCH:  5\n",
      "EPOCH:  6\n",
      "EPOCH:  7\n",
      "EPOCH:  8\n",
      "EPOCH:  9\n",
      "EPOCH:  10\n",
      "EPOCH:  11\n",
      "EPOCH:  12\n",
      "EPOCH:  13\n",
      "EPOCH:  14\n",
      "EPOCH:  15\n",
      "EPOCH:  16\n",
      "EPOCH:  17\n",
      "EPOCH:  18\n",
      "EPOCH:  19\n",
      "EPOCH:  20\n",
      "EPOCH:  21\n",
      "EPOCH:  22\n",
      "EPOCH:  23\n",
      "EPOCH:  24\n",
      "\n",
      " Epoch:  24\n",
      "System energy: -0.03499999999999983\n",
      "Generating Embeddings:  took  1334.2774987220764  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(holder)\n",
    "\n",
    "embeddings = ut.randomly_initialize_embedding_space(vocab_size, num_of_dims)\n",
    "\n",
    "learned_embeddings = model.pipeline_of_learning_embeddings(e=embeddings,\n",
    "                                                           max_iteration=bound_on_iter,\n",
    "                                                           energy_release_at_epoch=e_release,\n",
    "                                                           holder=holder, omega=omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime  24.841650541623434\n"
     ]
    }
   ],
   "source": [
    "minutes=(1334.2774987220764 + 156.2215337753296)/60\n",
    "\n",
    "print('Total runtime ',minutes) # rounded 25 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use memory efficiently\n",
    "del holder\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Type Prediction  starts ######\n",
      "K values: [1, 3, 5, 10, 15, 30, 50, 100]\n",
      "##### 1 ####\n",
      "Mean type prediction [0.96962103]\n",
      "##### 3 ####\n",
      "Mean type prediction [0.69831727]\n",
      "##### 5 ####\n",
      "Mean type prediction [0.62199829]\n",
      "##### 10 ####\n",
      "Mean type prediction [0.56098671]\n",
      "##### 15 ####\n",
      "Mean type prediction [0.48592903]\n",
      "##### 30 ####\n",
      "Mean type prediction [0.37220401]\n",
      "##### 50 ####\n",
      "Mean type prediction [0.27262127]\n",
      "##### 100 ####\n",
      "Mean type prediction [0.21259995]\n",
      "Type Prediction  took  7003.54514169693  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyser.perform_type_prediction(learned_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To see what happens under the hood\n",
    "\n",
    "\n",
    "```python\n",
    "@performance_debugger('Type Prediction')\n",
    "def perform_type_prediction(self, df):\n",
    "\n",
    "    def create_binary_type_vector(t_types, a_types):\n",
    "        vector = np.zeros(len(all_types))\n",
    "        i = [a_types.index(_) for _ in t_types]\n",
    "        vector[i] = 1\n",
    "        return vector\n",
    "\n",
    "    def create_binary_type_prediction_vector(t_types, a_types):\n",
    "        vector = np.zeros(len(all_types))\n",
    "        i = [a_types.index(_) for _ in itertools.chain.from_iterable(t_types)]\n",
    "        vector[i] += 1\n",
    "        return vector\n",
    "\n",
    "    # get the types. Mapping from the index of subject to the index of object\n",
    "    type_info = ut.deserializer(path=self.p_folder, serialized_name='type_info')\n",
    "\n",
    "    # get the index of objects / get type information =>>> s #type o\n",
    "    all_types = sorted(set.union(*list(type_info.values())))\n",
    "\n",
    "\n",
    "    # Consider only points with type infos.\n",
    "    e_w_types = df.loc[list(type_info.keys())]\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=101, algorithm='kd_tree', metric='euclidean', n_jobs=-1).fit(\n",
    "        e_w_types)\n",
    "\n",
    "    # Get similarity results for selected entities\n",
    "    df_most_similars = pd.DataFrame(neigh.kneighbors(e_w_types, return_distance=False))\n",
    "\n",
    "    # As sklearn implementation of kneighbors returns the point itself as most similar point\n",
    "    df_most_similars.drop(columns=[0], inplace=True)\n",
    "\n",
    "\n",
    "    # If kNN operProbabily no need to mapping as we calculated for all\n",
    "    mapper = dict(zip(list(range(len(e_w_types))), e_w_types.index.values))\n",
    "    # The values of most similars are mapped to original vocabulary positions\n",
    "    df_most_similars = df_most_similars.applymap(lambda x: mapper[x])\n",
    "\n",
    "\n",
    "    k_values = [1, 3, 5, 10, 15, 30, 50, 100]\n",
    "\n",
    "    print('K values:',k_values)\n",
    "    for k in k_values:\n",
    "        print('#####', k, '####')\n",
    "        similarities = list()\n",
    "        for _, S in df_most_similars.iterrows():\n",
    "            true_types = type_info[S.values[0]]\n",
    "            type_predictions = [type_info[_] for _ in S.values[1:k + 1]]\n",
    "\n",
    "            vector_true = create_binary_type_vector(true_types, all_types)\n",
    "            vector_prediction = create_binary_type_prediction_vector(type_predictions, all_types)\n",
    "\n",
    "            sim = cosine(vector_true, vector_prediction)\n",
    "            similarities.append(1 - sim)\n",
    "\n",
    "        report = pd.DataFrame(similarities)\n",
    "        print('Mean type prediction', report.mean().values)\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pl2vec)",
   "language": "python",
   "name": "pl2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
